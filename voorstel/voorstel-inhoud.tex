% !TeX program = xelatex
%---------- Inleiding ---------------------------------------------------------

% TODO: Is dit voorstel gebaseerd op een paper van Research Methods die je
% vorig jaar hebt ingediend? Heb je daarbij eventueel samengewerkt met een
% andere student?
% Zo ja, haal dan de tekst hieronder uit commentaar en pas aan.

%\paragraph{Opmerking}

% Dit voorstel is gebaseerd op het onderzoeksvoorstel dat werd geschreven in het
% kader van het vak Research Methods dat ik (vorig/dit) academiejaar heb
% uitgewerkt (met medesturent VOORNAAM NAAM als mede-auteur).
% 

\section{Inleiding}%
\label{sec:inleiding}

Softwareontwikkeling is een complex proces waarbij kwaliteit en snelheid vaak met elkaar in strijd zijn.
Een heel belangrijk deel tijdens het ontwikkelen van software dat zorgt voor het behouden van een hoge kwaliteit van de code is 'Code Review'.
Dat is het proces waarbij developers elkaars code controleren om na te kijken of deze geen fouten bevat, voordat ze het stukje gaan toevoegen aan de hoofdcode.

\subsection{Probleemstelling}
De meeste software development teams besteden ongeveer 20 procent van hun tijd aan handmatige code reviews.
Het reviewen van code is heel belangerijk, maar het kost de developers heel veel tijd en is vaak inconsistent.
Developers moeten vaak dagen wachten voordat hun \emph{pull request} (PR) gereviewd wordt,
waardoor dat ze vaak niet verder kunnen. De kwaliteit van de review is ook sterk afhankelijk van de ervaring van de reviewer,
waardoor sommige bugs gemist worden en later grotere bugs kunnen veroorzaken of de kwaliteit van de code verminderen.
Dit probleem is specifiek voor de doelgroep van dit onderzoek: \textbf{kleinere software development teams (tot 15 leden)},
waar capaciteit beperkt is.

\subsection{Onderzoeksvragen}
De centrale onderzoeksvraag is:
\textit{In welke mate kunnen geautomatiseerde code review tools de inconsistenties en bottlenecks in handmatige code reviews verminderen bij kleine software development teams?}

Om een antwoord te vinden op deze vraag, moeten we een aantal deelvragen beantwoorden uit zowel het probleemdomein als het oplossingsdomein.

\paragraph{Probleemdomein (Huidige situatie)}
\begin{enumerate}
    \item Wat is de gemiddelde doorlooptijd van een pull request tot 500 regels code, bij alleen handmatige reviews, en hoeveel procent van deze tijd is wachttijd?
    \item Welke specifieke review-activiteiten (lezen, commentaar schrijven, discuseren) nemen de meeste tijd in beslag?
    \item Wat is de \emph{defect detection rate} bij handmatige reviews: hoeveel fouten worden gemiddeld gevonden en welk type fouten wordt het meeste gemist?
\end{enumerate}

\paragraph{Oplossingsdomein (Automatisering)}
\begin{enumerate}
    \item Welke percentage van de bugs wordt gedetecteerd door geautomatiseerde code review tools in vergelijking met handmatige reviews?
    \item Hoeveel tijd in procent kan bespaard worden door geautomatiseerde tools te gebruiken voor de initiële fase van de review?
    \item Welke type fouten (bijv. security, code style, performance) worden sneller gedetecteerd door geautomatiseerde tools dan door mensen?
\end{enumerate}

\subsection{Onderzoeksdoelstelling}
Het doel van deze bachelorproef is om op basis van meetbare resultaten kleine teams te ondersteunen bij het verbeteren van hun code review proces.
Het eindresultaat zal aantonen welke combinatie van handmatige reviews en tools het meest efficiënt is.

%---------- Stand van zaken ---------------------------------------------------

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

Binnen de werled van softwareontwikkeling worden code reviews als essentieel gezien om een kwaliteitsvolle code te schrijven.
Uit onderzoek van \textcite{McIntosh2015} komen we te weten dat er een sterke correlatie is tussen de grondigheid van code reviews en de uiteindelijke kwaliteit van de software.
Teams die voor alle code reviews doen, hebben veel minder bugs in productie fase van een software waaraan ze aan het werken zijn.
Toch benadrukken \textcite{Bacchelli2013} dat de verwachtingen in de praktijk niet altijd overeenkomen met de realiteit.
Zij stellen vast dat de hoofddoel van code reviews wel het vinden van fouten is, maar eigenlijk is het vooral goed omdat verschillende developers zo kennis met elkaar kunnen delen.
Bovendien ondervonden ze dat tijdsdruk een groot probleem is, dat vaak zorgt voor inconsistente reviews en lange wachttijden.

Om al deze menselijke inconsistenties en grote tijdsverlies te vermijden, focussen recentere onderzoeken zich heel vaak op automatisering.
In een heel uitgebreide onderzoek naar de staat van moderne code reviews geven \textcite{Yang2024} aan dat moderne tools constant beter worden in het herkennen van complexe patronen,
die vroeger nog een porbleem waren om te detecteren en altijd menselijke ingrip nodig hadden. Deze evolutie naar betere tools wordt verder onderbouwd door \textcite{Almeida2023},
die met AI gebaseerde tools voor code reviews aantonen dat kwaliteit van de code verhoogd kan worden wanneer er gebruik gemaakt wordt van deze tools.

Het klinkt allemaal veelbelovend, maar momenteel zijn geautomatiseerde tools voor code reviews nog niet volledig perfect.
Uit recent onderzoek van \textcite{Cihan2024} blijkt dat developers nog vaak te maken krijgen met een hoog aantal \emph{false positives} (onterechte foutmeldingen) die de geautomatiseerde tools geven.
Dit zorgt ervoor dat de developers veel errors moeten nakijken, waarvan de meeste onterecht zijn, wat vaak leidt tot verlies van focus en wordt niet genoeg aandacht besteed en zware fouten.
Daarnaast merken \textcite{Yang2024} op dat het niet zo simpel is om deze tools te integreren in bestaande workflows en zeker niet wanneer de configuratie heel complex is.

%---------- Methodologie ------------------------------------------------------
\section{Methodologie}%
\label{sec:methodologie}

Dit onderzoek zal gebruik maken van een vergelijkende studie.

\subsection{Dataverzameling}
Er zal een dataset worden samengesteld van pull requests (uit een open-source project of van bepaalde bedrijven).

\subsection{Experiment}
Het experiment zal bestaan uit twee fases:
\begin{enumerate}
    \item \textbf{Handmatig:} De code wordt handmatig gereviewd door developers. Hierbij worden gaan we aantal dingen meten: tijd gespendeerd, aantal gevonden fouten, type fouten.
    \item \textbf{Automatisch:} Dezelfde code wordt geanalyseerd door geselecteerde tools (bijv. GitHub Copilot, CodeRabbit en SonarQube), ook hier meten we dezelfde gegevens.
\end{enumerate}

\subsection{Analyse}
De resultaten worden vergeleken op basis van kwantitatieve metrics:
\begin{itemize}
    \item \textbf{Efficiëntie:} Tijd (uitgedrukt in minuten) van pull request creatie tot merge.
    \item \textbf{Accuratie:} Aantal terechte foutmeldingen (True Positives) vs. valse meldingen (False Positives).
\end{itemize}

%---------- Verwachte resultaten ----------------------------------------------
\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

De verwachting is dat geautomatiseerde tools veel sneller en beter zullen zijn in het detecteren van syntax-, stijl- en eenvoudige beveiligingsfouten,
wat zal leiden tot een grote tijdsbesparing in de initiële fase van de review.
Echter, er wordt verwacht dat voor complexe logische fouten menselijke input nogsteeds nodig zal zijn.

Het eindresultaat zal gevisualiseerd worden in een grafiek die de gemiddelde reviewtijd vergelijkt
tussen het handmatige reviewen scenario en het megngeling van geautomatiseerde tools met menselijke input scenario.
